name: CI/CD Pipeline

on:
  # Trigger after Code Quality workflow completes
  workflow_run:
    workflows: ["Code Quality & Security"]
    types:
      - completed
    branches:
      - main
      - development
      - test-atlas-cicd
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test'
        required: true
        default: 'development'
        type: choice
        options:
          - development
          - staging
          - test
      run_performance_tests:
        description: 'Run performance tests'
        required: false
        default: true
        type: boolean
      run_e2e_tests:
        description: 'Run E2E tests'
        required: false
        default: true
        type: boolean
      skip_matrix_tests:
        description: 'Skip matrix testing (faster run)'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION_MATRIX: '[18, 20, 22]'

jobs:
  # Check if Code Quality passed (only for workflow_run trigger)
  check-code-quality:
    name: Check Code Quality Status
    runs-on: ubuntu-latest
    timeout-minutes: 2
    if: github.event_name == 'workflow_run'
    outputs:
      should_proceed: ${{ steps.check.outputs.should_proceed }}
    
    steps:
      - name: Check Code Quality workflow result
        id: check
        run: |
          if [[ "${{ github.event.workflow_run.conclusion }}" == "success" ]]; then
            echo "should_proceed=true" >> $GITHUB_OUTPUT
            echo "âœ… Code Quality checks passed, proceeding with CI/CD"
          else
            echo "should_proceed=false" >> $GITHUB_OUTPUT
            echo "âŒ Code Quality checks failed, skipping CI/CD"
            exit 1
          fi

  # Job 1: Setup and Basic Dependency Install
  setup:
    name: Setup Dependencies
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [check-code-quality]
    if: always() && (github.event_name == 'workflow_dispatch' || needs.check-code-quality.outputs.should_proceed == 'true')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            **/node_modules
          key: ${{ runner.os }}-npm-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-npm-

      - name: Install dependencies
        run: |
          npm ci
          cd frontend && npm ci
          cd ../backend && npm ci
          cd ../ai-service && npm ci

      - name: Verify installations
        run: |
          echo "âœ… All dependencies installed successfully"
          node --version
          npm --version

  # Job 2: Matrix Testing with Multiple Node.js Versions
  test-matrix:
    name: Test Matrix (Node ${{ matrix.node-version }}) - ${{ matrix.service }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: setup
    if: always() && needs.setup.result == 'success' && (github.event_name != 'workflow_dispatch' || github.event.inputs.skip_matrix_tests != 'true')
    
    strategy:
      matrix:
        node-version: [18, 20, 22]
        service: [frontend, backend, ai-service]
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            **/node_modules
          key: ${{ runner.os }}-node${{ matrix.node-version }}-${{ matrix.service }}-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node${{ matrix.node-version }}-${{ matrix.service }}-

      - name: Install dependencies
        working-directory: ./${{ matrix.service }}
        run: npm ci

      - name: Run tests with coverage
        working-directory: ./${{ matrix.service }}
        env:
          SKIP_DOCKER_TESTS: true
        run: npm run test:coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./${{ matrix.service }}/coverage/lcov.info
          flags: ${{ matrix.service }}-node${{ matrix.node-version }}
          name: ${{ matrix.service }}-coverage-node${{ matrix.node-version }}
          fail_ci_if_error: false

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.service }}-node${{ matrix.node-version }}
          path: |
            ./${{ matrix.service }}/coverage/
            ./${{ matrix.service }}/test-results.xml
          retention-days: 7

  # Job 3: Service Integration Testing (Atlas-enabled)
  e2e-tests:
    name: E2e Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [setup]
    if: always() && needs.setup.result == 'success' && (github.event_name != 'workflow_dispatch' || github.event.inputs.run_e2e_tests == 'true')
    
    # No local MongoDB services - using Atlas

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            **/node_modules
          key: ${{ runner.os }}-e2e-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-e2e-

      - name: Install dependencies
        run: |
          npm ci
          cd frontend && npm ci
          cd ../backend && npm ci
          cd ../ai-service && npm ci

      - name: Install Playwright
        working-directory: ./e2e-tests
        run: |
          npm ci
          npx playwright install --with-deps

      - name: Clean up ports
        run: |
          # Kill any processes using our target ports
          sudo lsof -ti:3000 | xargs -r sudo kill -9 || true
          sudo lsof -ti:3001 | xargs -r sudo kill -9 || true
          sudo lsof -ti:3002 | xargs -r sudo kill -9 || true
          echo "Ports cleaned up"

      - name: Start services
        run: |
          # Create backend log file
          mkdir -p logs
          
          # Start backend on port 3000 with Atlas
          echo "Starting backend service..."
          (cd backend && PORT=3000 DATABASE_URL="${{ secrets.TEST_DATABASE_URL }}" MONGODB_URI="${{ secrets.TEST_DATABASE_URL }}" JWT_SECRET=test_jwt_secret_key SESSION_SECRET=test_session_secret_key NODE_ENV=test npm start > ../logs/backend.log 2>&1) &
          BACKEND_PID=$!
          echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV
          echo "Backend started with PID: $BACKEND_PID"
          
          # Start AI service on port 3002
          echo "Starting AI service..."
          (cd ai-service && PORT=3002 GROQ_API_KEY=test_key_placeholder NODE_ENV=test npm start > ../logs/ai-service.log 2>&1) &
          AI_SERVICE_PID=$!
          echo "AI_SERVICE_PID=$AI_SERVICE_PID" >> $GITHUB_ENV
          echo "AI service started with PID: $AI_SERVICE_PID"
          
          # Start frontend on port 3001
          echo "Starting frontend service..."
          (cd frontend && NEXT_PUBLIC_BACKEND_URL=http://localhost:3000 npm run build > ../logs/frontend-build.log 2>&1 && PORT=3001 npm start > ../logs/frontend.log 2>&1) &
          FRONTEND_PID=$!
          echo "FRONTEND_PID=$FRONTEND_PID" >> $GITHUB_ENV
          echo "Frontend started with PID: $FRONTEND_PID"
          
          # Wait for services to be ready with better health checking
          echo "Waiting for services to start..."
          sleep 15
          
          # Check backend health multiple times
          for i in {1..10}; do
            echo "Health check attempt $i/10..."
            if curl -f http://localhost:3000/health; then
              echo "âœ… Backend is responding at http://localhost:3000/health"
              break
            elif curl -f http://localhost:3000/; then
              echo "âœ… Backend root endpoint is responding at http://localhost:3000/"
              break
            else
              echo "â³ Waiting for backend... (attempt $i/10)"
              if [ $i -eq 10 ]; then
                echo "âŒ Backend health check failed after 10 attempts"
                echo "Backend logs:"
                cat logs/backend.log || echo "No backend logs found"
                ps aux | grep -E "(node|npm)" | grep -v grep || echo "No Node processes found"
              fi
              sleep 3
            fi
          done
          
          # Check frontend
          echo "Checking frontend..."
          curl -f http://localhost:3001 || echo "âš ï¸ Frontend not ready"

      - name: Run E2E tests with Playwright
        working-directory: ./e2e-tests
        env:
          BASE_URL: http://localhost:3000
          FRONTEND_URL: http://localhost:3001
          NODE_ENV: test
          CI: true
        run: |
          # List available test files
          echo "Available test files:"
          ls -la tests/
          
          # Run the existing simple health test
          echo "Running simple health check tests..."
          npx playwright test tests/simple-health.spec.js --reporter=list

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            ./e2e-tests/test-results/
            ./e2e-tests/playwright-report/
            ./logs/
          retention-days: 7

      - name: Cleanup services
        if: always()
        run: |
          kill $BACKEND_PID $AI_SERVICE_PID $FRONTEND_PID 2>/dev/null || true

  # Job 4: Performance Testing (Atlas-enabled)
  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [setup]
    if: always() && needs.setup.result == 'success' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/development' || github.ref == 'refs/heads/test-atlas-cicd' || (github.event_name == 'workflow_dispatch' && github.event.inputs.run_performance_tests == 'true'))
    
    # No local MongoDB services - using Atlas
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm install -g artillery
          (cd frontend && npm ci)
          (cd backend && npm ci)
          (cd ai-service && npm ci)

      - name: Clean up ports for performance testing
        run: |
          # Kill any processes using our target ports
          sudo lsof -ti:3000 | xargs -r sudo kill -9 || true
          sudo lsof -ti:3001 | xargs -r sudo kill -9 || true
          sudo lsof -ti:3002 | xargs -r sudo kill -9 || true
          echo "Ports cleaned up for performance testing"

      - name: Start services for performance testing
        run: |
          # Create performance test log directory
          mkdir -p logs
          
          # Start backend on port 3000 with Atlas
          echo "Starting backend service for performance testing..."
          (cd backend && PORT=3000 DATABASE_URL="${{ secrets.TEST_DATABASE_URL }}" MONGODB_URI="${{ secrets.TEST_DATABASE_URL }}" JWT_SECRET=test_jwt_secret_key SESSION_SECRET=test_session_secret_key NODE_ENV=test npm start > ../logs/perf-backend.log 2>&1) &
          BACKEND_PID=$!
          echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV
          echo "Backend started with PID: $BACKEND_PID"
          
          # Start AI service on port 3002
          echo "Starting AI service for performance testing..."
          (cd ai-service && PORT=3002 GROQ_API_KEY=test_key_placeholder NODE_ENV=test npm start > ../logs/perf-ai-service.log 2>&1) &
          AI_SERVICE_PID=$!
          echo "AI_SERVICE_PID=$AI_SERVICE_PID" >> $GITHUB_ENV
          echo "AI service started with PID: $AI_SERVICE_PID"
          
          # Start frontend on port 3001
          echo "Starting frontend service for performance testing..."
          (cd frontend && NEXT_PUBLIC_BACKEND_URL=http://localhost:3000 npm run build > ../logs/perf-frontend-build.log 2>&1 && PORT=3001 npm start > ../logs/perf-frontend.log 2>&1) &
          FRONTEND_PID=$!
          echo "FRONTEND_PID=$FRONTEND_PID" >> $GITHUB_ENV
          echo "Frontend started with PID: $FRONTEND_PID"
          
          # Wait for services to be ready with health checking
          echo "Waiting for services to start for performance testing..."
          sleep 15
          
          # Check backend health for performance testing
          for i in {1..10}; do
            echo "Performance test health check attempt $i/10..."
            if curl -f http://localhost:3000/health; then
              echo "âœ… Backend is ready for performance testing"
              break
            elif curl -f http://localhost:3000/; then
              echo "âœ… Backend root endpoint is ready for performance testing"
              break
            else
              echo "â³ Waiting for backend... (attempt $i/10)"
              if [ $i -eq 10 ]; then
                echo "âŒ Backend health check failed for performance testing"
                echo "Performance backend logs:"
                cat logs/perf-backend.log || echo "No performance backend logs found"
              fi
              sleep 3
            fi
          done

      - name: Run performance tests
        run: |
          # Use the performance test configuration from new location
          artillery run tests/performance/performance-test.yml --output performance-report.json

      - name: Generate performance report
        run: |
          artillery report performance-report.json --output performance-report.html

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            performance-report.json
            performance-report.html
            logs/perf-*.log
          retention-days: 7

      - name: Cleanup services
        if: always()
        run: |
          kill $BACKEND_PID $AI_SERVICE_PID $FRONTEND_PID 2>/dev/null || true

  # Job 5: Test Summary
  test-summary:
    name: Test Summary Report
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [setup, test-matrix, e2e-tests, performance-test]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Generate test summary
        run: |
          echo "# Test Summary Report" > test-summary.md
          echo "Generated on: $(date)" >> test-summary.md
          echo "" >> test-summary.md
          
          # Check if test artifacts exist and summarize
          if [ -d "test-results-frontend-node20" ]; then
            echo "âœ… Frontend tests completed" >> test-summary.md
          else
            echo "âŒ Frontend tests failed" >> test-summary.md
          fi
          
          if [ -d "test-results-backend-node20" ]; then
            echo "âœ… Backend tests completed" >> test-summary.md
          else
            echo "âŒ Backend tests failed" >> test-summary.md
          fi
          
          if [ -d "e2e-test-results" ]; then
            echo "âœ… E2E tests completed" >> test-summary.md
          else
            echo "âŒ E2E tests failed" >> test-summary.md
          fi
          
          if [ -d "performance-results" ]; then
            echo "âœ… Performance tests completed" >> test-summary.md
          else
            echo "âŒ Performance tests failed or skipped" >> test-summary.md
          fi

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: test-summary.md
          retention-days: 30

  # Job 6: Notification
  notify:
    name: Send Notifications
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [setup, test-matrix, e2e-tests, performance-test, test-summary]
    if: always()
    
    steps:
      - name: Determine workflow status
        id: status
        run: |
          if [[ "${{ needs.test-summary.result }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "emoji=âœ…" >> $GITHUB_OUTPUT
            echo "message=All CI/CD checks passed successfully!" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "emoji=âŒ" >> $GITHUB_OUTPUT
            echo "message=Some CI/CD checks failed. Please review the results." >> $GITHUB_OUTPUT
          fi

      - name: Create workflow summary
        run: |
          echo "## ${{ steps.status.outputs.emoji }} CI/CD Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ steps.status.outputs.message }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Job Results:" >> $GITHUB_STEP_SUMMARY
          echo "- **Setup:** ${{ needs.setup.result || 'Skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Matrix:** ${{ needs.test-matrix.result || 'Skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **E2E Tests:** ${{ needs.e2e-tests.result || 'Skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Performance Tests:** ${{ needs.performance-test.result || 'Skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> ðŸ’¡ **Note:** Docker builds and deployments are handled by the separate [Deploy to Environments](../actions/workflows/deploy.yml) workflow."

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && comment.body.includes('CI/CD Pipeline Summary')
            );
            
            const body = `## ${{ steps.status.outputs.emoji }} CI/CD Pipeline Summary
            
            **Status:** ${{ steps.status.outputs.message }}
            **Branch:** ${{ github.ref_name }}
            **Commit:** ${{ github.sha }}
            
            ### Job Results:
            - **Setup:** ${{ needs.setup.result || 'Skipped' }}
            - **Test Matrix:** ${{ needs.test-matrix.result || 'Skipped' }}
            - **E2E Tests:** ${{ needs.e2e-tests.result || 'Skipped' }}
            - **Performance Tests:** ${{ needs.performance-test.result || 'Skipped' }}
            
            > ðŸ’¡ **Note:** Code quality checks are handled by [Code Quality & Security](../actions/workflows/code-quality.yml) and Docker builds/deployments by [Deploy to Environments](../actions/workflows/deploy.yml).`;
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }