name: CI/CD Pipeline

on:
  workflow_run:
    workflows: ["Code Quality & Security"]
    types:
      - completed
    branches:
      - main
      - development
  workflow_dispatch:

env:
  NODE_VERSION_MATRIX: '[18, 20, 22]'

jobs:
  # Check if Code Quality passed (only for workflow_run trigger)
  check-code-quality:
    name: Check Code Quality Status
    runs-on: ubuntu-latest
    timeout-minutes: 2
    if: github.event_name == 'workflow_run'
    outputs:
      should_proceed: ${{ steps.check.outputs.should_proceed }}
    
    steps:
      - name: Check Code Quality workflow result
        id: check
        run: |
          if [[ "${{ github.event.workflow_run.conclusion }}" == "success" ]]; then
            echo "should_proceed=true" >> $GITHUB_OUTPUT
            echo "âœ… Code Quality checks passed, proceeding with CI/CD"
          else
            echo "should_proceed=false" >> $GITHUB_OUTPUT
            echo "âŒ Code Quality checks failed, skipping CI/CD"
            exit 1
          fi

  # Job 1: Setup and Basic Dependency Install
  setup:
    name: Setup Dependencies
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [check-code-quality]
    if: always() && (github.event_name == 'workflow_dispatch' || needs.check-code-quality.outputs.should_proceed == 'true')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            **/node_modules
          key: ${{ runner.os }}-npm-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-npm-

      - name: Install dependencies
        run: |
          npm ci
          cd frontend && npm ci
          cd ../backend && npm ci
          cd ../ai-service && npm ci

      - name: Verify installations
        run: |
          echo "âœ… All dependencies installed successfully"
          node --version
          npm --version

  # Job 2: Matrix Testing with Multiple Node.js Versions
  test-matrix:
    name: Test Matrix (Node ${{ matrix.node-version }})
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: setup
    
    strategy:
      matrix:
        node-version: [18, 20, 22]
        service: [frontend, backend, ai-service]
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            **/node_modules
          key: ${{ runner.os }}-node${{ matrix.node-version }}-${{ matrix.service }}-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node${{ matrix.node-version }}-${{ matrix.service }}-

      - name: Install dependencies
        working-directory: ./${{ matrix.service }}
        run: npm ci

      - name: Run tests with coverage
        working-directory: ./${{ matrix.service }}
        env:
          SKIP_DOCKER_TESTS: true
        run: npm run test:coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./${{ matrix.service }}/coverage/lcov.info
          flags: ${{ matrix.service }}-node${{ matrix.node-version }}
          name: ${{ matrix.service }}-coverage-node${{ matrix.node-version }}
          fail_ci_if_error: false

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.service }}-node${{ matrix.node-version }}
          path: |
            ./${{ matrix.service }}/coverage/
            ./${{ matrix.service }}/test-results.xml
          retention-days: 7

  # Job 3: Service Integration Testing
  e2e-tests:
    name: E2e Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [setup, test-matrix]
    
    services:
      mongo:
        image: mongo:4.4
        ports:
          - 27017:27017
        env:
          MONGO_INITDB_DATABASE: brainbytes
        options: >-
          --health-cmd "echo 'db.runCommand(\"ping\").ok' | mongo localhost:27017/test --quiet"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            **/node_modules
          key: ${{ runner.os }}-e2e-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-e2e-

      - name: Install dependencies
        run: |
          npm ci
          cd frontend && npm ci
          cd ../backend && npm ci
          cd ../ai-service && npm ci

      - name: Install Playwright
        working-directory: ./e2e-tests
        run: |
          npm ci
          npx playwright install --with-deps

      - name: Clean up ports
        run: |
          # Kill any processes using our target ports
          sudo lsof -ti:3000 | xargs -r sudo kill -9 || true
          sudo lsof -ti:3001 | xargs -r sudo kill -9 || true
          sudo lsof -ti:3002 | xargs -r sudo kill -9 || true
          echo "Ports cleaned up"

      - name: Start services
        run: |
          # Start backend on port 3000
          (cd backend && PORT=3000 MONGODB_URI=mongodb://localhost:27017/brainbytes JWT_SECRET=test_jwt_secret_key SESSION_SECRET=test_session_secret_key NODE_ENV=test npm start) &
          BACKEND_PID=$!
          echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV
          
          # Start AI service on port 3002
          (cd ai-service && PORT=3002 GROQ_API_KEY=test_key_placeholder NODE_ENV=test npm start) &
          AI_SERVICE_PID=$!
          echo "AI_SERVICE_PID=$AI_SERVICE_PID" >> $GITHUB_ENV
          
          # Start frontend on port 3001
          (cd frontend && NEXT_PUBLIC_BACKEND_URL=http://localhost:3000 npm run build && PORT=3001 npm start) &
          FRONTEND_PID=$!
          echo "FRONTEND_PID=$FRONTEND_PID" >> $GITHUB_ENV
          
          # Wait for services to be ready
          sleep 30
          
          # Check if services are running
          echo "Checking backend health..."
          curl -f http://localhost:3000/api/health || echo "Backend not ready"
          echo "Checking frontend..."
          curl -f http://localhost:3001 || echo "Frontend not ready"

      - name: Run E2E tests with Playwright
        working-directory: ./e2e-tests
        run: |
          # Create a simple health check test
          cat > tests/simple-health.spec.js << 'EOF'
          const { test, expect } = require('@playwright/test');

          test('Backend API is responding', async ({ request }) => {
            const response = await request.get('http://localhost:3000/');
            expect(response.status()).toBe(200);
            const data = await response.json();
            expect(data.message).toContain('BrainBytes API');
          });

          test('Frontend is responding', async ({ page }) => {
            await page.goto('http://localhost:3001');
            await expect(page).toHaveTitle(/BrainBytes/);
          });
          EOF
          
          # Run the simple tests
          npx playwright test tests/simple-health.spec.js

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            ./e2e-tests/test-results/
            ./e2e-tests/playwright-report/
          retention-days: 7

      - name: Cleanup services
        if: always()
        run: |
          kill $BACKEND_PID $AI_SERVICE_PID $FRONTEND_PID 2>/dev/null || true

  # Job 4: Performance Testing
  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [test-matrix]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm install -g artillery
          (cd frontend && npm ci)
          (cd backend && npm ci)
          (cd ai-service && npm ci)

      - name: Clean up ports for performance testing
        run: |
          # Kill any processes using our target ports
          sudo lsof -ti:3000 | xargs -r sudo kill -9 || true
          sudo lsof -ti:3001 | xargs -r sudo kill -9 || true
          sudo lsof -ti:3002 | xargs -r sudo kill -9 || true
          echo "Ports cleaned up for performance testing"

      - name: Start services for performance testing
        run: |
          # Start backend on port 3000
          (cd backend && PORT=3000 MONGODB_URI=mongodb://localhost:27017/brainbytes JWT_SECRET=test_jwt_secret_key SESSION_SECRET=test_session_secret_key NODE_ENV=test npm start) &
          BACKEND_PID=$!
          echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV
          
          # Start AI service on port 3002
          (cd ai-service && PORT=3002 GROQ_API_KEY=test_key_placeholder NODE_ENV=test npm start) &
          AI_SERVICE_PID=$!
          echo "AI_SERVICE_PID=$AI_SERVICE_PID" >> $GITHUB_ENV
          
          # Start frontend on port 3001
          (cd frontend && NEXT_PUBLIC_BACKEND_URL=http://localhost:3000 npm run build && PORT=3001 npm start) &
          FRONTEND_PID=$!
          echo "FRONTEND_PID=$FRONTEND_PID" >> $GITHUB_ENV
          
          # Wait for services to be ready
          sleep 30

      - name: Run performance tests
        run: |
          # Create basic artillery config
          cat > performance-test.yml << EOF
          config:
            target: 'http://localhost:3000'
            phases:
              - duration: 60
                arrivalRate: 10
          scenarios:
            - name: "API Load Test"
              requests:
                - get:
                    url: "/health"
          EOF
          
          artillery run performance-test.yml --output performance-report.json

      - name: Generate performance report
        run: |
          artillery report performance-report.json --output performance-report.html

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            performance-report.json
            performance-report.html
          retention-days: 7

      - name: Cleanup services
        if: always()
        run: |
          kill $BACKEND_PID $AI_SERVICE_PID $FRONTEND_PID 2>/dev/null || true

  # Job 5: Test Summary
  test-summary:
    name: Test Summary Report
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [test-matrix, e2e-tests, performance-test]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Generate test summary
        run: |
          echo "# Test Summary Report" > test-summary.md
          echo "Generated on: $(date)" >> test-summary.md
          echo "" >> test-summary.md
          
          # Check if test artifacts exist and summarize
          if [ -d "test-results-frontend-node20" ]; then
            echo "âœ… Frontend tests completed" >> test-summary.md
          else
            echo "âŒ Frontend tests failed" >> test-summary.md
          fi
          
          if [ -d "test-results-backend-node20" ]; then
            echo "âœ… Backend tests completed" >> test-summary.md
          else
            echo "âŒ Backend tests failed" >> test-summary.md
          fi
          
          if [ -d "e2e-test-results" ]; then
            echo "âœ… E2E tests completed" >> test-summary.md
          else
            echo "âŒ E2E tests failed" >> test-summary.md
          fi
          
          if [ -d "performance-results" ]; then
            echo "âœ… Performance tests completed" >> test-summary.md
          else
            echo "âŒ Performance tests failed or skipped" >> test-summary.md
          fi

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: test-summary.md
          retention-days: 30

  # Job 6: Notification
  notify:
    name: Send Notifications
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [test-summary]
    if: always()
    
    steps:
      - name: Determine workflow status
        id: status
        run: |
          if [[ "${{ needs.test-summary.result }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "emoji=âœ…" >> $GITHUB_OUTPUT
            echo "message=All CI/CD checks passed successfully!" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "emoji=âŒ" >> $GITHUB_OUTPUT
            echo "message=Some CI/CD checks failed. Please review the results." >> $GITHUB_OUTPUT
          fi

      - name: Create workflow summary
        run: |
          echo "## ${{ steps.status.outputs.emoji }} CI/CD Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ steps.status.outputs.message }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Job Results:" >> $GITHUB_STEP_SUMMARY
          echo "- **Setup:** ${{ needs.setup.result || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Matrix:** ${{ needs.test-matrix.result || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **E2E Tests:** ${{ needs.e2e-tests.result || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Performance Tests:** ${{ needs.performance-test.result || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> ðŸ’¡ **Note:** Docker builds and deployments are handled by the separate [Deploy to Environments](../actions/workflows/deploy.yml) workflow."

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && comment.body.includes('CI/CD Pipeline Summary')
            );
            
            const body = `## ${{ steps.status.outputs.emoji }} CI/CD Pipeline Summary
            
            **Status:** ${{ steps.status.outputs.message }}
            **Branch:** ${{ github.ref_name }}
            **Commit:** ${{ github.sha }}
            
            ### Job Results:
            - **Setup:** ${{ needs.setup.result || 'N/A' }}
            - **Test Matrix:** ${{ needs.test-matrix.result || 'N/A' }}  
            - **E2E Tests:** ${{ needs.e2e-tests.result || 'N/A' }}
            - **Performance Tests:** ${{ needs.performance-test.result || 'N/A' }}
            
            > ðŸ’¡ **Note:** Code quality checks are handled by [Code Quality & Security](../actions/workflows/code-quality.yml) and Docker builds/deployments by [Deploy to Environments](../actions/workflows/deploy.yml).`;
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }